{
    "classification": {
        "model": "MedCLIP",
        "status": "completed",
        "output": "Setting up paths...\nLoading image...\nCreating processor...\nPreparing inputs...\nLoading MedCLIP model...\nload model weight from: ./pretrained/medclip-vit\nCreating classifier...\nUsing GPU: NVIDIA GeForce GTX 1650 Ti\nPreparing classification prompts...\nsample 10 num of prompts for Atelectasis from total 210\nsample 10 num of prompts for Cardiomegaly from total 15\nsample 10 num of prompts for Consolidation from total 192\nsample 10 num of prompts for Edema from total 18\nsample 10 num of prompts for Pleural Effusion from total 54\nRunning classification...\n\n============================================================\nMEDCLIP CLASSIFICATION RESULTS\n============================================================\nModel Confidence Scores:\n----------------------------------------\n 1. Atelectasis          0.207 ****\n 2. Cardiomegaly         0.232 ****\n 3. Consolidation        0.169 ***\n 4. Edema                0.181 ***\n 5. Pleural Effusion     0.212 ****\n\n----------------------------------------\nMost Likely Condition: Cardiomegaly\nConfidence: 0.232\n\nRaw Logits Values:\nShape: torch.Size([1, 5])\nValues: [0.357461541891098, 0.4732661843299866, 0.1539868414402008, 0.22492043673992157, 0.383976548910141]\n\nDetailed Analysis:\n----------------------------------------\n 1. Atelectasis          Logit: 0.3575\n 2. Cardiomegaly         Logit: 0.4733\n 3. Consolidation        Logit: 0.1540\n 4. Edema                Logit: 0.2249\n 5. Pleural Effusion     Logit: 0.3840\n\n"
    },
    "segmentation": {
        "model": "MedCLIP-SAMv2",
        "status": "failed",
        "error": "Traceback (most recent call last):\n  File \"D:\\Study\\HCMUS\\Fourth_year\\KLTN\\source\\model\\MedCLIP-SAMv2\\run.py\", line 242, in <module>\n    medclip_sam_pipeline()\n  File \"D:\\Study\\HCMUS\\Fourth_year\\KLTN\\source\\model\\MedCLIP-SAMv2\\run.py\", line 231, in medclip_sam_pipeline\n    medclip_classification(image_path, text)\n  File \"D:\\Study\\HCMUS\\Fourth_year\\KLTN\\source\\model\\MedCLIP-SAMv2\\run.py\", line 205, in medclip_classification\n    inputs = processor(\n  File \"C:\\Users\\phat8\\miniconda3\\envs\\medclip-sam\\lib\\site-packages\\transformers\\models\\clip\\processing_clip.py\", line 103, in __call__\n    image_features = self.image_processor(images, return_tensors=return_tensors, **kwargs)\n  File \"D:\\Study\\HCMUS\\Fourth_year\\KLTN\\source\\model\\MedCLIP\\medclip\\dataset.py\", line 108, in __call__\n    images = [self.pad_img(image,min_size=self.size) for image in images]\n  File \"D:\\Study\\HCMUS\\Fourth_year\\KLTN\\source\\model\\MedCLIP\\medclip\\dataset.py\", line 108, in <listcomp>\n    images = [self.pad_img(image,min_size=self.size) for image in images]\n  File \"D:\\Study\\HCMUS\\Fourth_year\\KLTN\\source\\model\\MedCLIP\\medclip\\dataset.py\", line 138, in pad_img\n    size = max(min_size, x, y)\nTypeError: '>' not supported between instances of 'int' and 'dict'\n\nERROR conda.cli.main_run:execute(127): `conda run python model/MedCLIP-SAMv2/run.py` failed. (See above for error)\n"
    },
    "timestamp": "D:\\Study\\HCMUS\\Fourth_year\\KLTN\\source",
    "image_path": "model/MedCLIP/example_data/view1_frontal.jpg"
}